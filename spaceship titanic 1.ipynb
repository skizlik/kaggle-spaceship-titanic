{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592377ce",
   "metadata": {},
   "source": [
    "# Spaceship Titanic Kaggle Competition Attempt 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b565e",
   "metadata": {},
   "source": [
    "## Basic Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xg\n",
    "\n",
    "# sklearn stuff\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc, accuracy_score, log_loss, classification_report,confusion_matrix,roc_curve,roc_auc_score\n",
    "\n",
    "# will be doing some optimization I'm sure\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61daa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a08b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f69da7",
   "metadata": {},
   "source": [
    "It looks at a casual glance like we have more variables than in the Titanic challenge.  Also, more of them are quantitative.  Interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_list = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NaN's by column?\n",
    "total = 0\n",
    "for column in train_data.columns:\n",
    "    print(\"variable:\", column, \"NaN count:\", train_data[column].isna().sum())\n",
    "    total += train_data[column].isna().sum()\n",
    "print(\"total NaN's:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ba6a8",
   "metadata": {},
   "source": [
    "This is problematic.  We have 8693 observations, and 2324 total NaN's.  Probably, there are some rows with multiple NaN's, but at any rate, this is simply too many to discard. __Particularly__ considering this is a Kaggle competition.  It seems obvious to me that this challenge is very much about imputation of missing values.  Of course there are a variety of ways to go about this, I'm going to try to approach it systematically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about the test set?\n",
    "total = 0\n",
    "for column in test_data.columns:\n",
    "    print(\"variable:\", column, \"NaN count:\", test_data[column].isna().sum())\n",
    "    total += test_data[column].isna().sum()\n",
    "print(\"total NaN's:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808575f",
   "metadata": {},
   "source": [
    "So the missing values will be a problem, too, for the test set.  Simply ignoring all records with NaN's isn't even a possible (though bad) solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it may be relevant whether people are travelling in parties\n",
    "# multiple people with same last name might be a reasonable proxy for this\n",
    "\n",
    "train_data[[\"FirstName\", \"LastName\"]]=train_data[\"Name\"].str.split(\" \", expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aa98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data[\"Name\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c072a22",
   "metadata": {},
   "source": [
    "Just wanted to make sure that the null values propagated across that split, instead of something... weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"LastName\"].value_counts()[\"Susent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f66269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[\"LastNameCount\"] = train_data[\"LastName\"].value_counts()[train_data[\"LastName\"]]\n",
    "# produces KeyError: '[nan] not in index'\n",
    "\n",
    "train_data[\"LastNameCount\"] = train_data.groupby(\"LastName\")[\"LastName\"].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf656a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2dc7a7",
   "metadata": {},
   "source": [
    "We also have some useful data in the deceptively informative 'Cabin' field.  From the Kaggle competition page:\n",
    "\n",
    "Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "    \n",
    "This should be broken up into three columns...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[[\"Deck\", \"Num\", \"Side\"]]=train_data[\"Cabin\"].str.split(\"/\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns out that 'PassengerId' consists of a group number and a passenger number...\n",
    "train_data[[\"Group\", \"Passenger\"]]=train_data[\"PassengerId\"].str.split(\"_\", expand=True)\n",
    "train_data[\"GroupCount\"]=train_data.groupby(\"Group\")[\"Group\"].transform('count')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488b0de",
   "metadata": {},
   "source": [
    "Of course, I'm going to need to expand the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7862575",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[[\"Deck\", \"Num\", \"Side\"]]=test_data[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "test_data[[\"FirstName\", \"LastName\"]]=test_data[\"Name\"].str.split(\" \", expand=True)\n",
    "test_data[\"LastNameCount\"] = test_data.groupby(\"LastName\")[\"LastName\"].transform('count')\n",
    "test_data[[\"Group\", \"Passenger\"]]=test_data[\"PassengerId\"].str.split(\"_\", expand=True)\n",
    "test_data[\"GroupCount\"]=test_data.groupby(\"Group\")[\"Group\"].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b0dc2",
   "metadata": {},
   "source": [
    "It occurs to me that the \"LastNameCount\" should arguably be aggregated across both the test and training sets; I'll come back to consider this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa15016",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f0632",
   "metadata": {},
   "source": [
    "It also occurs to me that all of the actual information in the \"Cabin\" and \"Name\" columns now exists in other columns.  This will cause fitting problems with several methods, and of course is just redundant.  If the original variables are ever needed, they can easily be reconstructed.  Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f744293",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop([\"Cabin\", \"Name\"], axis=1, inplace=True)\n",
    "test_data.drop([\"Cabin\", \"Name\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80072751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12714e57",
   "metadata": {},
   "source": [
    "Ok, looks like I'm ready to proceed to some basic EDA, which will inform imputation (I suspect this will be important here) and model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70de60",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these may very well be needed here\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf832143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with value counts...\n",
    "vc_list = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"LastNameCount\", \"GroupCount\", \"Deck\", \"Side\"]\n",
    "for col in vc_list:\n",
    "    print(\"variable:\", col)\n",
    "    print(train_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c208b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare value counts across the outcome variable, \"Transported\":\n",
    "for col in vc_list:\n",
    "    print(\"variable:\", col)\n",
    "    print(train_data.groupby(\"Transported\")[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ad50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps it will be more informative to reverse the order of the groupby and value counts:\n",
    "for col in vc_list:\n",
    "    print(train_data.groupby(col)[\"Transported\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37444d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be lengthy, but...\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "train_data.groupby(\"LastName\")[\"Transported\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10740afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(\"FirstName\")[\"Transported\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dff54",
   "metadata": {},
   "source": [
    "I was hoping to see a lot more names that were only true or only false.  That's not to say there isn't an association with names, there could very well be.  I should consider what kind of metric might be applicable here.\n",
    "\n",
    "However, some of these variables can be visualized usefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8927ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(\"HomePlanet\")[\"Transported\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = train_data.groupby(\"HomePlanet\")[\"Transported\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e80c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c11644",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50db567",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vc.iloc[::2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964202c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis = np.arange(len(vc.index)/2)\n",
    "plt.bar(X_axis - 0.2, vc[vc.index.get_level_values('Transported') == False], 0.4, label=\"Not Transported\")\n",
    "plt.bar(X_axis + 0.2, vc[vc.index.get_level_values('Transported') == True], 0.4, label=\"Transported\")\n",
    "plt.xticks(X_axis, ['Earth', 'Europa', 'Mars'])\n",
    "plt.xlabel(\"HomePlanet\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ff989",
   "metadata": {},
   "source": [
    "That worked nicely, and on the first try - can I generalize it?  Side by side bar graphs should serve well for the categorical variables here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in vc_list:\n",
    "    vc = train_data.groupby(col)[\"Transported\"].value_counts()\n",
    "    X_axis = np.arange(len(vc.index)/2)\n",
    "    X_axis_list = []\n",
    "    for i in np.arange(len(vc.index)/2):\n",
    "        X_axis_list.append(vc.iloc[::2].index[i][0])\n",
    "    plt.bar(X_axis - 0.2, vc[vc.index.get_level_values('Transported') == False], 0.4, label=\"Not Transported\")\n",
    "    plt.bar(X_axis + 0.2, vc[vc.index.get_level_values('Transported') == True], 0.4, label=\"Transported\")\n",
    "    plt.xticks(X_axis, X_axis_list)\n",
    "    plt.xlabel(col)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebce632",
   "metadata": {},
   "source": [
    "Great!  It would have been tedious to do all of those manually.  There are some interesting points to note in passing:  HomePlanet seems to have a substantial effect, at least with respect to Earth v Europa.  Obviously, CryoSleep makes Transported far more likely, too.  Destination appears to have something of a weaker effect....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a7954",
   "metadata": {},
   "source": [
    "I'm curious, though, about the LastNameCount variable - is there significant evidence that these counts are not from the same distribution?  It's been a while since I've run a formal hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnc_t = train_data[train_data[\"Transported\"] == True].LastNameCount.value_counts().sort_index()\n",
    "lnc_f = train_data[train_data[\"Transported\"] == False].LastNameCount.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnc_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lnc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnc_chi_sq = pd.concat([lnc_t, lnc_f], axis=1)\n",
    "lnc_chi_sq.columns = (['Transported', 'Not Transported'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ce279",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnc_chi_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "r = scipy.stats.chisquare(lnc_chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb476e",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, these test statistics are indeed so large as to give a p-value of, effectively, zero... I suppose that when considering the number of observations this should have been obvious.  At any rate, it's reasonable to expect this variable to have some predictive power.\n",
    "\n",
    "It might also be informative to do a little similar basic EDA for the quantitative variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72349188",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = train_data['Age'][train_data['Transported'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in quant_list:\n",
    "    tt = train_data[col][train_data[\"Transported\"] == True]\n",
    "    nt = train_data[col][train_data[\"Transported\"] == False]\n",
    "    plt.hist([tt, nt], label=[\"Transported\", \"Not Transported\"])\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaba644",
   "metadata": {},
   "source": [
    "These histograms are simply _atrocious_ - but I'm not terribly interested in perfecting them.  We can see some variation with respect to age.  The amount spent on various amenities generally seems to increase the chances of transportation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be92a4",
   "metadata": {},
   "source": [
    "# Imputation of Missing Values\n",
    "\n",
    "As discussed above, all potentially relevant predictors have a nontrivial proportion of missing values - too many to discard.  Imputation is clearly an important component of this competition, and I'd like to approach it in an appropriate manner.\n",
    "\n",
    "It might also be interesting to compare results from different categories of imputation methods.  A complication, of course, is that the same method must be applied to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff62dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn imputer a good starting point\n",
    "# you could make a set of columns for logs of shopping mall, etc\n",
    "# when submitting code to git, reset kernel and clear outputs -- easiest to say git push, but...\n",
    "# get in the habit of looking for duplicate rows!  and also look for outliers\n",
    "# when you do take home tests, make sure you have a blurb about each of these...\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "# apparently required for IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# apparently IterativeImputer isn't stable yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2954e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's an important question:  should I impute values for the train and test sets separately, or together?\n",
    "# maybe I should compare both... let's do seperately first\n",
    "\n",
    "# of course, the parameters are going to depend on the data type\n",
    "# and I refuse to put together a dataframe of dataframes again, for this thing\n",
    "# so I'm going to need a naming convention\n",
    "# TrainSepSimp, TestSepSimp, TrainSepKnn, TestSepKnn, TrainSepIter, TestSepIter\n",
    "# TrainPoolSimp, TestPoolSimp, TrainPoolKnn, TestPoolKnn, TrainPoolIter, TestPoolIter\n",
    "\n",
    "# I have made more work for myself than need be, perhaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:  \n",
    "    print(col, train_data[col].isna().sum(), type(train_data[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test_data.columns:\n",
    "    print(col, test_data[col].isna().sum(), type(test_data[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63007c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to need to use different imputation metrics on different columns, and I don't think SimpleImputer\n",
    "# will do this automatically, if the docs are any indication\n",
    "\n",
    "quant_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"LastNameCount\", \"GroupCount\"]\n",
    "cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Num\", \"Side\", \"FirstName\", \"LastName\", \"Group\", \"Passenger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ce86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainSepSimp, TestSepSimp, TrainSepKnn, TestSepKnn, TrainSepIter, TestSepIter\n",
    "# TrainPoolSimp, TestPoolSimp, TrainPoolKnn, TestPoolKnn, TrainPoolIter, TestPoolIter\n",
    "# again, why am I doing this?  A normal person would pick one method and move on...\n",
    "TrainSepSimp = pd.DataFrame()\n",
    "TestSepSimp = pd.DataFrame()\n",
    "for col in quant_cols:\n",
    "    imp = SimpleImputer(strategy='median') # few of these looked remotely symmetric\n",
    "    TrainSepSimp[col] = imp.fit_transform(train_data[col].values.reshape(-1,1))[:,0]\n",
    "    TestSepSimp[col] = imp.fit_transform(test_data[col].values.reshape(-1,1))[:,0]\n",
    "  \n",
    "    \n",
    "for col in cat_cols:\n",
    "    imp = SimpleImputer(strategy = 'most_frequent')  # why the heck not 'mode'?\n",
    "    TrainSepSimp[col] = imp.fit_transform(train_data[col].values.reshape(-1,1))[:,0]\n",
    "    TestSepSimp[col] = imp.fit_transform(test_data[col].values.reshape(-1,1))[:,0]\n",
    "\n",
    "TrainSepSimp['Transported'] = train_data['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ed6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8691c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSepKnn = pd.DataFrame()\n",
    "TestSepKnn = pd.DataFrame()\n",
    "for col in quant_cols:\n",
    "    imp = KNNImputer(n_neighbors=3)\n",
    "    TrainSepKnn[col] = imp.fit_transform(train_data[col].values.reshape(-1,1))[:,0]\n",
    "    TestSepKnn[col] = imp.fit_transform(test_data[col].values.reshape(-1,1))[:,0]\n",
    "  \n",
    "    \n",
    "for col in cat_cols:\n",
    "    imp = SimpleImputer(strategy = 'most_frequent')\n",
    "    TrainSepKnn[col] = imp.fit_transform(train_data[col].values.reshape(-1,1))[:,0]\n",
    "    TestSepKnn[col] = imp.fit_transform(test_data[col].values.reshape(-1,1))[:,0]\n",
    "\n",
    "TrainSepKnn['Transported'] = train_data['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSepIter = pd.DataFrame()\n",
    "TestSepIter = pd.DataFrame()\n",
    "for col in quant_cols:\n",
    "    imp = IterativeImputer(random_state=0)\n",
    "    TrainSepIter[col] = imp.fit_transform(train_data[col].values.reshape(-1,1))[:,0]\n",
    "    TestSepIter[col] = imp.fit_transform(test_data[col].values.reshape(-1,1))[:,0]\n",
    "  \n",
    "    \n",
    "for col in cat_cols:\n",
    "    imp = SimpleImputer(strategy = 'most_frequent')\n",
    "    TrainSepIter[col] = imp.fit_transform(train_data[col].values.reshape(-1,1))[:,0]\n",
    "    TestSepIter[col] = imp.fit_transform(test_data[col].values.reshape(-1,1))[:,0]\n",
    "\n",
    "TrainSepIter['Transported'] = train_data['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pooling these, I'm going to need to unpool them later...\n",
    "test_data_p = test_data\n",
    "test_data_p[\"Transported\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29904406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_p = train_data\n",
    "train_data_p[\"TrainTest\"] = \"train\"\n",
    "test_data_p[\"TrainTest\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1290f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_data = pd.concat([train_data_p, test_data_p], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c148a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainPoolSimp, TestPoolSimp, TrainPoolKnn, TestPoolKnn, TrainPoolIter, TestPoolIter\n",
    "PoolSimp = pd.DataFrame()\n",
    "PoolSimp[[\"Transported\", \"TrainTest\"]] = pooled_data[[\"Transported\", \"TrainTest\"]]\n",
    "TrainPoolSimp = pd.DataFrame()\n",
    "TestPoolSimp = pd.DataFrame()\n",
    "for col in quant_cols:\n",
    "    imp = SimpleImputer(strategy='median') # few of these looked remotely symmetric\n",
    "    PoolSimp[col] = imp.fit_transform(pooled_data[col].values.reshape(-1,1))[:,0]\n",
    "\n",
    "    \n",
    "for col in cat_cols:\n",
    "    imp = SimpleImputer(strategy = 'most_frequent')  # why the heck not 'mode'?\n",
    "    PoolSimp[col] = imp.fit_transform(pooled_data[col].values.reshape(-1,1))[:,0]\n",
    "    \n",
    "TrainPoolSimp = PoolSimp[PoolSimp[\"TrainTest\"] == \"train\"]\n",
    "TestPoolSimp = PoolSimp[PoolSimp[\"TrainTest\"] == \"test\"]\n",
    "TrainPoolSimp.drop(\"TrainTest\", axis=1, inplace=True)\n",
    "TestPoolSimp.drop(\"TrainTest\", axis=1, inplace=True)\n",
    "TestPoolSimp.drop(\"Transported\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fcf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainPoolSimp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395f199",
   "metadata": {},
   "source": [
    "I'm stopping this, because it's just _boring_, maybe I'll come back and handle it later, for comparison... I'd like to just make a decent model first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b867894",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will be useful to store results\n",
    "results_cols = ['model type', 'imputation', 'hyperparameters', 'f1', 'roc_auc', 'accuracy']\n",
    "# it's my understanding that submissions will be judged on accuracy\n",
    "results = pd.DataFrame(columns = results_cols)\n",
    "# TrainSepSimp, TestSepSimp, TrainSepKnn, TestSepKnn, TrainSepIter, TestSepIter\n",
    "# 'imputation' will be 'simple', 'knn', or 'iterative'\n",
    "\n",
    "# since this is a binary outcome ('transported') I plan to use:\n",
    "# logistic regression, kNN, random forest, gaussian naive bayes, and the MLP neural net classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = list(TrainSepSimp.columns)\n",
    "# why can't you pass a list to list.remove?\n",
    "for col in ['FirstName', 'LastName', 'Transported']:\n",
    "    pred_cols.remove(col)\n",
    "pred_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed318d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f14a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with simple imputation\n",
    "# training and 'validation' sets as test set is for submission to be assessed\n",
    "\n",
    "X = TrainSepSimp[pred_cols]\n",
    "X[[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]] = X[[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]].apply(LabelEncoder().fit_transform)\n",
    "y = TrainSepSimp[\"Transported\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=13013)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "ra = roc_auc_score(y_val, y_pred)\n",
    "item = [\"Logistic regression\", \"simple\", \"N/A\", f1, ra, accuracy]\n",
    "itemdict = dict(zip(results_cols, item))\n",
    "results=results.append(itemdict, ignore_index=True)\n",
    "print(\"Logistic regression\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exactly the same but with knn imputation\n",
    "# training and 'validation' sets as test set is for submission to be assessed\n",
    "\n",
    "X = TrainSepKnn[pred_cols]\n",
    "X[[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]] = X[[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]].apply(LabelEncoder().fit_transform)\n",
    "y = TrainSepKnn[\"Transported\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=13013)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "ra = roc_auc_score(y_val, y_pred)\n",
    "item = [\"Logistic regression\", \"knn\", \"N/A\", f1, ra, accuracy]\n",
    "itemdict = dict(zip(results_cols, item))\n",
    "results=results.append(itemdict, ignore_index=True)\n",
    "print(\"Logistic regression\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76329ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and again, with iterative imputation\n",
    "# starting with simple imputation\n",
    "# training and 'validation' sets as test set is for submission to be assessed\n",
    "\n",
    "X = TrainSepIter[pred_cols]\n",
    "X[[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]] = X[[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]].apply(LabelEncoder().fit_transform)\n",
    "y = TrainSepIter[\"Transported\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=13013)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "ra = roc_auc_score(y_val, y_pred)\n",
    "item = [\"Logistic regression\", \"iterative\", \"N/A\", f1, ra, accuracy]\n",
    "itemdict = dict(zip(results_cols, item))\n",
    "results=results.append(itemdict, ignore_index=True)\n",
    "print(\"Logistic regression\\n\", \"Accuracy:\", accuracy, \"f1:\", f1, \"roc_auc:\", ra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a134d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a808e",
   "metadata": {},
   "source": [
    "It's worrisome that in all cases, we have a failure of convergence.  Interestingly, the knn and iterative imputation methods produced the same assessment metrics for logistic regression.  Is it possible that these imputers yielded exactly the same results?  How can I check this?  It would be silly to go and fit models to both datasets if they're identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7043cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
